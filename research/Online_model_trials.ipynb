{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online model checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Online Model checking -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anoop\\anaconda3\\envs\\mchatbot\\lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "streaming was transfered to model_kwargs.\n",
      "                    Please confirm that streaming is what you intended.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import Replicate\n",
    "\n",
    "llm = Replicate(\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    model=\"meta/llama-2-7b:acdbe5a4987a29261ba7d7d4195ad4fa6b62ce27b034f989fcb9ab0421408a7c\",\n",
    "    input={\"temperature\": 0.75, \"max_length\": 500, \"top_p\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY,\n",
    "              environment=PINECONE_API_ENV)\n",
    "\n",
    "index_name=\"statistics-chatbot\"\n",
    "docsearch=Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result [Document(page_content='Page 6 \\n \\nSH                                                       Statistics for Data Science  \\n \\nStatistics: \\nStatistics is a branch of mathematics that deals with the study of collecting, \\nanalysing, interpreting, presenting, and organizing data in a particular manner. Statistics \\nis defined as the process of collection of data, classifying data, representing the data for \\neasy interpretation, and further analysis of data. Statistics also is referred to as arriving', metadata={}), Document(page_content='at conclusions from the sample data that is collected using surveys or experiments. \\nDifferent sectors such as psychology, sociology, geology, probability, and so on also \\nuse statistics to function. \\n \\nMathematical Statistics: \\nStatistics is used mainly to gain an understanding of the data and focus on \\nvarious applications. Statistics is the process of collecting data, evaluating data, and \\nsummarizing it into a mathematical form. Initially, statistics were related to the science', metadata={}), Document(page_content='of the state where it was used in the collection and analysis of facts and data about a \\ncountry such as its economy, population, etc. Mathematical statistics applies \\nmathematical techniques like linear algebra, differential equations, mathematical \\nanalysis, and theories of probability. \\nThere are two methods of analysing data in mathematical statistics that are used \\non a large scale: \\n \\nDescriptive Statistics: \\nThe descriptive method of statistics is used to describe the data collected and', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "index_name=\"statistics-chatbot\"\n",
    "\n",
    "docsearch=Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "query = \"What is Statistics?\"\n",
    "\n",
    "docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response :  I have worked on this project before so I can share with you some interesting insights.\n",
      "Here are some considerations when training a new network on a new dataset.\n",
      "**First,** `the number of layers` in your network will affect the performance of it.\n",
      "In general, if we compare a neural net of two different depths, always expect more complex nets perform better. The reason behind that is pretty simple: More layers means there should be more parameters involved (both nn.param() and dropout).\n",
      "To build a good model, we need many more hyperparameters than we think.\n",
      "For example, I find that *bigger batch size* makes our \\strong{training convergence faster}. So we could use bigger learning rate as well.\n",
      "However, speed up doesnâ€™t mean accuracy improvement. In other words, we may not always want our models converge slower because of lower precision loss. For example, we might want to train networks whose prediction confidence is higher but more imprecise. In such case, smaller minibatches would help us here.\n",
      "\n",
      "The second important factor is how much time spent on the training procedure depends on the used algorithms. For instance, Stochastic Gradient Descent (SGD) converges very fast in comparison with Adam optimizer though SGD has one disadvantage - higher chance for over/under-fitting.\n",
      "Second, we often let the network train itself on test data during fitting. This is called \\strong{test set} or \\strong{validation set}, it is usually smaller than testing set which contains all examples from the whole dataset. Those validation examples are important only at the end stage of the training process and their purpose is to check whether the chosen architecture generalizes well enough. It would also be wise to check whether your model works stably under different conditions like changing input shape etc.\n",
      "Then comes the question about how many epochs should you expect to run through? Usually, a fair number is around 50 - we don't want too few since they will not be able to learn anything useful, neither do we need too large ones that make the whole process take days without getting any results.\n",
      "Next thing worth considering is model regularization: There are various types of regularizers like *L2* or *L1*, they can reduce overfitted models by forcing them to represent some aspects of data better while minimizing others.\n"
     ]
    }
   ],
   "source": [
    "user_input=input(f\"Input Prompt:\")\n",
    "result=qa({\"query\": user_input})\n",
    "print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response :  Stats are one of the tools we use to understand data. It includes collecting, analyzing, interpreting, and making graphs from raw data (in this case, you).\n",
      "\n",
      "Answer: 42\n",
      "*/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=qa({\"query\": \"What is stats?\"})\n",
    "print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
